import streamlit as st
import pandas as pd
import joblib
import numpy as np

from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    matthews_corrcoef,
    confusion_matrix,
    classification_report,
    roc_auc_score
)

# ------------------------------
# Page Title
# ------------------------------
st.title("Multiclass Classification Model Evaluation App")

st.write("Upload test dataset (CSV) and evaluate selected model.")

# ------------------------------
# Model Selection
# ------------------------------
model_option = st.selectbox(
    "Select Model",
    (
        "Logistic Regression",
        "Decision Tree",
        "KNN",
        "Naive Bayes",
        "Random Forest",
        "XGBoost"
    )
)

# ------------------------------
# Load Selected Model
# ------------------------------
def load_model(model_name):
    if model_name == "Logistic Regression":
        return joblib.load("model/logistic_regression.pkl")
    elif model_name == "Decision Tree":
        return joblib.load("model/decision_tree.pkl")
    elif model_name == "KNN":
        return joblib.load("model/k_nearest_neighbor.pkl")
    elif model_name == "Naive Bayes":
        return joblib.load("model/gaussian_naive_bayes.pkl")
    elif model_name == "Random Forest":
        return joblib.load("model/random_forest.pkl")
    elif model_name == "XGBoost":
        return joblib.load("model/xgboost.pkl")

# ------------------------------
# File Upload
# ------------------------------
uploaded_file = st.file_uploader("Upload Test CSV File", type=["csv"])

if uploaded_file is not None:
    
    df = pd.read_csv(uploaded_file)

    st.subheader("Uploaded Dataset Preview")
    st.write(df.head())

    # Assume last column is target
    X = df.iloc[:, :-1]
    y = df.iloc[:, -1]

    model = load_model(model_option)

    # Prediction
    y_pred = model.predict(X)
    y_prob = model.predict_proba(X)

    # ------------------------------
    # Evaluation Metrics
    # ------------------------------
    accuracy = accuracy_score(y, y_pred)
    precision = precision_score(y, y_pred, average='weighted')
    recall = recall_score(y, y_pred, average='weighted')
    f1 = f1_score(y, y_pred, average='weighted')
    mcc = matthews_corrcoef(y, y_pred)
    auc = roc_auc_score(y, y_prob, multi_class='ovr')

    st.subheader("Evaluation Metrics")

    st.write(f"Accuracy: {accuracy:.4f}")
    st.write(f"Precision: {precision:.4f}")
    st.write(f"Recall: {recall:.4f}")
    st.write(f"F1 Score: {f1:.4f}")
    st.write(f"MCC: {mcc:.4f}")
    st.write(f"AUC Score: {auc:.4f}")

    # ------------------------------
    # Confusion Matrix
    # ------------------------------
    st.subheader("Confusion Matrix")

    cm = confusion_matrix(y, y_pred)
    st.write(cm)

    # ------------------------------
    # Classification Report
    # ------------------------------
    st.subheader("Classification Report")
    report = classification_report(y, y_pred)
    st.text(report)